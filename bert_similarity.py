# -*- coding: utf-8 -*-
"""bert_similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mIk0J-wsvCiP_Jl-b0KYJNdPYoXrQ2nV
"""

!pip install -U sentence-transformers

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from nltk import sent_tokenize
import nltk
nltk.download('punkt')

from sentence_transformers import SentenceTransformer

qpost_data = pd.read_csv("/content/drive/Shareddrives/Data Fest 2023/data/reducedQuestions.csv")

qpost_data["PostText"][25]
database = qpost_data.drop(25, axis = 0)
database["PostText"][24]

target = qpost_data["PostText"][25]
compare_base = database["PostText"]

def process_bert_similarity():
	# This will download and load the pretrained model offered by UKPLab.
	model = SentenceTransformer('bert-base-nli-mean-tokens')

	# Although it is not explicitly stated in the official document of sentence transformer, the original BERT is meant for a shorter sentence. We will feed the model by sentences instead of the whole documents.
	sentences = sent_tokenize(target)
	base_embeddings_sentences = model.encode(sentences)
	base_embeddings = np.mean(np.array(base_embeddings_sentences), axis=0)

	vectors = []
	for i, question in enumerate(compare_base):

		sentences = sent_tokenize(question)
		embeddings_sentences = model.encode(sentences)
		embeddings = np.mean(np.array(embeddings_sentences), axis=0)

		vectors.append(embeddings)

		print("making vector at index:", i)

	scores = cosine_similarity([base_embeddings], vectors).flatten()

	highest_score = 0
	highest_score_index = 0
	for i, score in enumerate(scores):
		if highest_score < score:
			highest_score = score
			highest_score_index = i

	most_similar_question = question[highest_score_index]
	print("Most similar question by BERT with the score:", most_similar_question, highest_score)

process_bert_similarity()

